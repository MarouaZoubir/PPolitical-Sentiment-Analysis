{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mtranslate"
      ],
      "metadata": {
        "id": "bGhPp_mrEo24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import datetime"
      ],
      "metadata": {
        "id": "n_ytd5y8cnfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def is_valid_url(url):\n",
        "    \"\"\"Validate the URL to ensure it's in the correct format.\"\"\"\n",
        "    return url.startswith(\"https://\")\n",
        "\n",
        "def fetch_article_urls(base_url, num_pages=1):\n",
        "    \"\"\"Scrape article URLs from the Hespress website.\"\"\"\n",
        "    article_urls = []\n",
        "    headers = {\n",
        "        'User-Agent': (\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
        "        )\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}/page/{page}\" if page > 1 else base_url\n",
        "        try:\n",
        "            response = session.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "                links = soup.find_all(\"a\", class_=\"stretched-link\")\n",
        "                for link in links:\n",
        "                    href = link.get(\"href\")\n",
        "                    if href and is_valid_url(href):\n",
        "                        article_urls.append(href)\n",
        "            else:\n",
        "                print(f\"Failed to fetch page {page} with status code {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while fetching page {page}: {e}\")\n",
        "\n",
        "        # Add a delay between requests to avoid being blocked\n",
        "        time.sleep(2)\n",
        "\n",
        "    return list(set(article_urls))  # Remove duplicates\n",
        "\n",
        "# Replace 'https://www.hespress.com' with the base URL of the website\n",
        "base_url = \"https://www.hespress.com/politique\"\n",
        "num_pages = 5  # Number of pages to scrape\n",
        "\n",
        "article_urls = fetch_article_urls(base_url, num_pages)\n",
        "\n",
        "if article_urls:\n",
        "    print(f\"Found {len(article_urls)} articles.\")\n",
        "    for url in article_urls:\n",
        "        print(url)\n",
        "else:\n",
        "    print(\"No articles found.\")\n"
      ],
      "metadata": {
        "id": "WT306TicQb3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def is_valid_url(url):\n",
        "    \"\"\"Validate the URL to ensure it's in the correct format.\"\"\"\n",
        "    return url.startswith(\"https://\")\n",
        "\n",
        "def fetch_article_urls(base_url, num_pages=1):\n",
        "    \"\"\"Scrape article URLs from the Hespress website.\"\"\"\n",
        "    article_urls = []\n",
        "    headers = {\n",
        "        'User-Agent': (\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
        "        )\n",
        "    }\n",
        "    session = requests.Session()\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}/page/{page}\" if page > 1 else base_url\n",
        "        try:\n",
        "            response = session.get(url)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "                links = soup.find_all(\"a\", class_=\"stretched-link\")\n",
        "                for link in links:\n",
        "                    href = link.get(\"href\")\n",
        "                    if href and is_valid_url(href):\n",
        "                        article_urls.append(href)\n",
        "            else:\n",
        "                print(f\"Failed to fetch page {page} with status code {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while fetching page {page}: {e}\")\n",
        "\n",
        "        # Add a delay between requests to avoid being blocked\n",
        "        time.sleep(2)\n",
        "\n",
        "    return list(set(article_urls))  # Remove duplicates\n",
        "\n",
        "# Replace 'https://www.hespress.com/politique' with the base URL of the website\n",
        "base_url = \"https://www.hespress.com/politique\"\n",
        "num_pages = 5  # Number of pages to scrape\n",
        "\n",
        "article_urls = fetch_article_urls(base_url, num_pages)\n",
        "\n",
        "# Print the list of URLs\n",
        "if article_urls:\n",
        "    print(f\"Found {len(article_urls)} articles.\")\n",
        "    for url in article_urls:\n",
        "        print(url)\n",
        "else:\n",
        "    print(\"No articles found.\")\n"
      ],
      "metadata": {
        "id": "RVrsJj6ZR2QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "from mtranslate import translate\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def is_valid_url(url):\n",
        "    try:\n",
        "        result = urlparse(url)\n",
        "        if all([result.scheme, result.netloc]) and result.netloc == \"www.hespress.com\":\n",
        "            return True\n",
        "        return False\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def arabic_to_english_month(arabic_month):\n",
        "    months_arabic = ['يناير', 'فبراير', 'مارس', 'أبريل', 'ماي', 'يونيو', 'يوليوز', 'غشت', 'شتنبر', 'أكتوبر', 'نونبر', 'دجنبر']\n",
        "    months_english = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "    return months_english[months_arabic.index(arabic_month)]\n",
        "\n",
        "def fetch_comments(url):\n",
        "    user_name = []\n",
        "    comment_text = []\n",
        "    comment_date = []\n",
        "    likes = []\n",
        "\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    data = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(data.content, \"html.parser\")\n",
        "\n",
        "    article_title_tag = soup.find('h1', class_='post-title')\n",
        "    article_title = article_title_tag.get_text() if article_title_tag else 'Unknown Title'\n",
        "\n",
        "    c = soup.find('ul', {\"class\": \"comment-list hide-comments\"})\n",
        "\n",
        "    if c:\n",
        "        for li in c.find_all('li', class_='comment'):\n",
        "            user_span = li.find('span', class_='fn heey')\n",
        "            user_name.append(user_span.get_text() if user_span else 'Unknown')\n",
        "\n",
        "            comment_p = li.find('p')\n",
        "            comment_text.append(comment_p.get_text() if comment_p else 'No comment text found')\n",
        "\n",
        "            date_span = li.find('div', class_='comment-date')\n",
        "            if date_span:\n",
        "                date_string = date_span.get_text().strip()\n",
        "                date_parts = date_string.split()\n",
        "                day = int(date_parts[1])\n",
        "                month_arabic = date_parts[2]\n",
        "                month = arabic_to_english_month(month_arabic)\n",
        "                year = int(date_parts[3])\n",
        "                time_parts = date_parts[-1].split(':')\n",
        "                hour = int(time_parts[0])\n",
        "                minute = int(time_parts[1])\n",
        "                comment_date.append(pd.Timestamp(year, datetime.strptime(month, '%B').month, day, hour, minute))\n",
        "            else:\n",
        "                comment_date.append('Unknown date')\n",
        "\n",
        "            likes_span = li.find('span', class_='comment-recat-number')\n",
        "            likes.append(int(likes_span.get_text()) if likes_span else 0)\n",
        "    else:\n",
        "        print(f\"No comments found for URL: {url}\")\n",
        "        return pd.DataFrame(columns=['User Name', 'Comment', 'Date', 'Likes']), article_title\n",
        "\n",
        "    df = pd.DataFrame({'User Name': user_name, 'Comment': comment_text, 'Date': comment_date, 'Likes': likes})\n",
        "    return df, article_title\n",
        "\n",
        "def analyze_sentiment(comment):\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    arabic_stopwords = set(stopwords.words('arabic'))\n",
        "    arabic_punctuation = set(string.punctuation)\n",
        "\n",
        "    try:\n",
        "        translated_comment = translate(comment, 'en')\n",
        "        tokens = translated_comment.split()\n",
        "        filtered_tokens = [word for word in tokens if word.lower() not in arabic_stopwords and word not in arabic_punctuation]\n",
        "        preprocessed_comment = ' '.join(filtered_tokens)\n",
        "        scores = sid.polarity_scores(preprocessed_comment)\n",
        "        compound_score = scores['compound']\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during sentiment analysis: {e}\")\n",
        "        return 'Neutral', 0.0\n",
        "\n",
        "    if compound_score >= 0.1:\n",
        "        return 'Positive', compound_score\n",
        "    elif compound_score <= -0.1:\n",
        "        return 'Negative', compound_score\n",
        "    else:\n",
        "        return 'Neutral', compound_score\n",
        "\n",
        "def create_bar_plot(comments_df):\n",
        "    sentiment_counts = comments_df['Sentiment'].value_counts()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "    plt.title('Sentiment Distribution')\n",
        "    plt.xlabel('Sentiment')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('static/images/sentiment_distribution.png')\n",
        "\n",
        "def create_time_series_plot(comments_df):\n",
        "    comments_df['Date'] = pd.to_datetime(comments_df['Date'], errors='coerce')\n",
        "    comments_df = comments_df.dropna(subset=['Date'])\n",
        "    comments_df.set_index('Date', inplace=True)\n",
        "    comments_over_time = comments_df.resample('D').size()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    comments_over_time.plot()\n",
        "    plt.title('Number of Comments Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Comments')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('static/images/comments_over_time.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    urls = article_urls\n",
        "\n",
        "\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for url in urls:\n",
        "        if is_valid_url(url):\n",
        "            comments_df, article_title = fetch_comments(url)\n",
        "            if not comments_df.empty:\n",
        "                comments_df['Sentiment'], comments_df['Sentiment Score'] = zip(*comments_df['Comment'].apply(analyze_sentiment))\n",
        "                combined_df = pd.concat([combined_df, comments_df])\n",
        "                print(f\"Comments for '{article_title}' have been fetched and analyzed.\")\n",
        "            else:\n",
        "                print(f\"No comments found for the article: {article_title}\")\n",
        "        else:\n",
        "            print(f\"Invalid URL: {url}\")\n",
        "\n",
        "    if not combined_df.empty:\n",
        "        combined_df.to_csv('comments_combined.csv', index=False)\n",
        "        print(\"All comments have been saved to 'comments_combined.csv'\")\n",
        "        create_bar_plot(combined_df)\n",
        "        create_time_series_plot(combined_df)\n",
        "    else:\n",
        "        print(\"No valid comments to analyze.\")"
      ],
      "metadata": {
        "id": "MK_6ZD_mSEkV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}